\chapter{Conclusion}
The following questions were posed in the introduction as a goal for the project:
\begin{enumerate}
	\renewcommand{\theenumi}{\alph{enumi}}
	\item Can the simulation be implemented in CUDA? Easily? \label{list:conclusion-ease}
	\item Which issues are there when mapping the problem to CUDA? Which solutions or alternatives are available? \label{list:conclusion-issues}
	\item Which parts of the simulation turn out to be critical paths? Can these parts of the code be improved further? \label{list:conclusion-bottleneck}
\end{enumerate}
The answer to \ref{list:conclusion-ease} is yes, it is entirely possible to implement particle simulations in CUDA. The
CUDA architecture it very well suited for three dimensional computing as a result of it's origin in computer graphics,
and with a large number of libraries available the implementation need not be to complex. Whether it is easy to
implement largely depends on ones point of view. The implementation is a relatively straightforward translation of math
to code, but indexing pitched arrays and managing indexes can be difficult, and often makes up the majority of a kernel.
The cuFFT provides a useful framework for GPU-accelerated FFTs, but requires some learning and setup to get working.
Compared to OpenCL and MPI a CUDA implementation might be easier to develop, but will be far less portable.

Issues encountered during development are mainly those common to developing for CUDA, such as ensuring coalesced memory
access, avoiding write conflicts, and minimize PCIe traffic. Issues related to programming include staying within bounds
of arrays, handling pointers correctly, and identifying sources of errors.

As might be expected the solver step of the simulation turned out to be the most compute intensive step. For cuFFT there
is only so much one can do to improve performance, but among criteria listed by Nvidia are using single precision data
types to reduce bandwidth cost, and keeping transform dimensions a multiple of low primes, ideally a power of two. For
the SOR kernel we can use shared memory to reduce memory access time, switch to single precision data types, and
identify the lowest number of iterations necessary for the solution to converge with a sufficiently low error. How to
distribute elements among warps need to be investigated to find out how to best take advantage of shared memory.

\paragraph{To conclude,} the author's opinion is that CUDA can be used to accelerate particle simulations with relatively few issues.
The performance of a simulation depends on the accuracy required, more so than for a similar simulations running on the
CPU. To get a better measure of potential speedup using CUDA, more optimization of the code is necessary, as well as a
comparison with CPU code running on similar hardware.

\section{Future Work}
As discussed, the code is not very optimized, lacking even proper use of shared memory where applicable. In addition to
this changing data types from single to double precision and fusing kernels are opportunities that should be investigate.
Algorithm wise there may be some gain in devising some clever particle sorting scheme, but impact of this appears
relatively low compared to the more critical solver performance. See section \ref{sec:discussion-flaws} for details on
suggested improvements.

The particle tracing mechanism could likely be improved to minimize performance impact, and more easily facilitate
further processing. This includes both how particles are traced during simulation and how trace data is output after
the simulation has ended (see also section \ref{sec:implementation-tracing}). Real time animation of particles and
electric field would also be interesting.

Further tests have been suggested in section \ref{sec:testing-further}, some of these assuming the performance
improvements above have been implemented.