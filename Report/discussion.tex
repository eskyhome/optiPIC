\chapter{Discussion}
%==%
\section{Results}
Some important characteristics noted during testing were:
\begin{itemize}
	\item The main factor in determining runtime is the grid resolution.
	\item The number of particles has a comparatively small performance impact.
	\item Kernel run times are relatively constant for increasing resolution.
	\item The vast majority of the computational work is done in the solver step.
	\item Our SOR solver has a high constant factor, but runtime remains more or less constant for different grid resolutions.
	\item GPU memory size is a limiting factor as far as problem size goes, preventing grid resolution much beyond $256^3$ for
	mid-end cards, and about $512^3$ for high-end cards. Particle number also has a noticeable impact on memory usage.
\end{itemize}

That execution time of charge distribution kernel decreases with larger resolution might seem counterintuitive, but since it is
called on a per-particle basis an increase in number of grid elements will have no impact on the work load. However,
because the charge distribution kernel writes to some grid element based on the particles' position, this makes fewer
particles write to the same element. By reducing these conflicts threads no longer have to wait for access, and thus
performance is improved.

Since the kernels don't appear to scale poorly with resolution the answer must be to improve the solver's scaling. As
the implementation ensures data alignment regardless of $n_x, n_y$ and $n_z$, and cuFFT is noted to perform best for
dimensions on the form $2^a \cdot 3^b \cdot 5^c \cdot 7^d$, it seems prudent to enforce this restriction on the grid
resolution. Even for values on this form, even ones tend to perform better than odd ones, suggesting that having a power
of two as a factor is desirable.

Even so, the FFT is an $O(n\cdot log\,n)$ complexity algorithm, while the SOR is relatively unaffected by the
resolution. Since the SOR is done in-place memory is much less of a factor than for the FFT solver, and it handles large grids
much better. As an element only reads from its neighbors, there is no increase in workload per thread when the number of
elements is increased. With the number of SOR iterations fixed at 128 runtime seems almost constant at 25 ms, other than
between

While resolution is still limited by memory, a measure that mey increase performance tenfold while reducing memory
footprint is to switch from double to single precision floating point values. For the GeForce series of GPUs the number
of operations per clock cycle is significantly lower for double precision. For compute capability 5.0 128 single precision
operations compared to 1 double precision, while for 3.0 the numbers are 192 and 8 \cite[sec.~5.4.1]{cudaprog}.
In addition to freeing up memory for additional resolution, there is a marked increase in processing power, utilizing
far more of the potential of the GPU.

While this holds true for consumer-grade GPUs, the Tesla series of accelerators has far better double precision
performance, with those of compute capability 3.5 capable of running 64 double precision operations per cycle.
Though, if single precision data turns out to be sufficiently accurate, the benefit of reduced memory footprint
may still justify the trade off. An interesting test in this regard would be to compare the accuracy loss of switching to
single precision versus the gain from increasing resolution. Testing this implementation on a Tesla card would also be
interesting, given the card's double precision performance.

%==%
\section{Implementation flaws, and potential fixes}\label{sec:discussion-flaws}
Listing some of the flaws to take into account when evaluating the implementation:
\begin{itemize}
	\item No use of shared memory. Important when working with CUDA, avoiding global memory accesses by utilizing the much
	faster on-chip shared memory, this should be one of the first improvements worked on. Especially the SOR-kernel should
	gain performance this way.
	\item Kernel fusion. The update particle and distribute charge kernels are called sequentially, with no dependencies on
	the first to finish execution before starting the second, and one could easily avoid expensive API calls by fusing these
	kernels together. Looking at the performance metrics however, The performance benefit would be minimal compared to
	optimizing the solver. Whether the code becomes more or less organized by fusing them is another matter.
	\item The red-black SOR currently consists of two consecutive kernel calls with an offset of one element. While this
	makes the implementation easy to understand a way of fusing these calls should be investigated. A argument in favor of
	the current implementation is that kernel calls are a trivial way to ensure the device wide synchronization needed.
	\item Particle sorting. Currently the particles maintain order in the particle array, and the same particles are put in
	a warp together every iteration. This might be wasteful, since particles on opposite sides of the grid might execute
	together, thus leading to scattered memory access. By sorting particles so that particles close together are executed
	together, we might get coalesced memory access, or even avoid different warps reading the same data. When writing data
	in the charge distribution kernel we would like to reduce the number of writes to global memory by handling writes to
	the same element in shared memory. See Elster's original work\cite[sec.~4.4]{elster94} for more details on particle sorting.

\end{itemize}

%==%
\section{Parallelizing PIC codes for CUDA}

Overall the particle-in-cell simulation seems well suited for parallel computing. All steps can be parallelized, and
there is little time spent in sequential parts of the code. Considering Gustafson's law (sec. \ref{sec:background-speedup})
we see that one can easily increase the parallel work without scaling the serial part, thus allowing unlimited speedup
in theory. Limiting factors are of course physical memory and processor throughput.

More specifically for the CUDA architecture, the mapping of a simulation step seems intuitive. Especially the grid-based
kernels map easily, given the GPU's predisposition towards 3D coordinates and stencil based computation. An issue here
is with the pitched pointers used to ensure data alignment. Using pitched pointers means accessing data using pointer
arithmetic that produces cluttered code. While this means we have fine grained control over data location, it also
leaves more room for error, and pointer calculation was indeed a major source of bugs during development. Additionally,
if we restrict the grid dimensions to a power of two for optimal FFT performance, data alignment is already taken care
of (assuming sufficiently large dimensions), and normal arrays could be used instead. 

If we want a grid resolution larger than what is possibly on GPU memory alone there is the possibility of storing the
complete grid in host memory and transferring back and forth every iteration, but this will likely become prohibitively
expensive considering the amount of data to be transferred. Other options are upgrading to a larger GPU memory, or
adding more GPU's to the system. By storing parts of the grid on different devices we only need to transfer values on the
border between them. Communication speed between devices is still an issue, but technologies like Nvidia's \href{http://www.nvidia.com/object/nvlink.html}{NVLink}
shows potential in this regard, promising speeds of up to 5 to 12 times what we would get over the PCIe 3 bus \cite{nvlink}.
For a 2 GPU 3D FFT of size $256^3$ the speedup of using NVLink is apparently nearly 2.25 over a PCIe configuration.

That the cuFFT library is readily available is another benefit of using CUDA. In CUDA version 6.5 callback functions
were added to FFT execution calls, since one usually follows a transform with some operation on the transformed data.
This way one avoids shifting control to the CPU before running the solver kernel, and using this for both the forward
transform-solver and inverse transform-electric field pairs of kernels, we avoid two of these calls per iteration.
The cuFFT library currently only supports acceleration on two GPUs, so this limits hardware scaling somewhat if this
library is to be used. An additional limit as that the entire transform must fit in memory of the GPU's involved\cite[sec.~2.8.4]{cufft-doc}.

The SOR also translates nicely to CUDA, especially if we optimize it using shared memory. One bottleneck is the error
checking, where the current implementation instead uses a fixed number of iteration. Checking whether error is
sufficiently small for all elements requires a global maximum reduction. Since this can be expensive for a large number
of elements, finding a number of iterations that is sufficient for convergence is a preferable solution. As the amount
of work done by the kernel is relatively low, an increase in the number of iterations may well outweigh the cost of
calculating the error every iteration.

%==%
\section{Solvers}
A straightforward comparison of the solvers' performance would be unfair considering that cuFFT is a highly optimizing
library while the SOR makes no use of shared memory yet. For all grid sizes 128 and below the FFT-based solver
completely outperforms the SOR, with results for 64 and below being an order of magnitude lower. It is therefore
surprising to see that the SOR has a lower runtime for $n=256$, about half that of the FFT. In addition the SOR can be
expected to handle irregular (even prime) grid dimensions far better, and since cuFFT resorts to a slower less accurate
algorithm for large primes \cite[sec.~2.12]{cufft-doc}, a lot of the difference would be made up even for smaller grid
sizes.

If the SOR is upgraded with shared memory one should also identify the best possible configuration of elements per warp/
block. Since an element reads three values from its own row plus an element from the rows above, below, in front and
behind, a total of five rows must be read to compute a value. By letting a warp handle a single row we reduce the number of coalesced
memory access to five rows. As an example lets compare configurations of threads per warp of (8, 8, 4) and (256, 1, 1),
both handling 256 elements. (8, 8, 4) reads eight elements from $8\times 4 = 32$ rows, all of which are likely scattered across
the array. (256, 1, 1) has coalesced reads from it's own row, plus four others for a total of five. Assuming dimensions
of $256^3$ this will also make the upper middle and lower rows successive in memory, resulting in only three scattered
accesses.

Since the FFT is quite communication intensive the performance benefit of switching to single precision data types is
likely greater than for the SOR kernel, and a new comparison using an optimized SOR kernel and single precision data is
warranted. Nevertheless, both solvers seem to perform comparably for problem sizes fitting in GPU memory, and the choice
is still one of selecting the one appropriate given the boundary conditions of the problem.