\chapter{Results}
This section will present some results from testing, in the form of 2D and 3D plots as appropriate, along with a brief
interpretation of the data.

All results are timed as described in section \ref{sec:testing-methodology}. Python with numpy and matplotlib is used to
present the data in the diagrams below. Runtime is measured in milliseconds, and unless otherwise specified is the time
taken for one iterations of the simulation, or one run through the tested code.

\section{Number of iterations}
\testresult{iterations2}{Simulation loop runtime as a function of $N_{iterations}$.}

As be expected the simulation runtime scaled linearly with the number of iterations, as can be seen on figure \ref{result:iterations2}.
Runtime was approximately $1.35\frac{\mathit{milliseconds}}{\mathit{iteration}}$.

\section{Grid resolution}
\testresult{grid1}{Simulation iteration runtime as a function of grid resolution. Isotropic.}
\testresult{grid2}{Runtime as a function of grid resolution, with scaling along z axis.}
\testresult{grid3b}{Runtime as a function of grid resolution, isotropic, odd valued size.}
\testresult{grid3a}{Runtime as a function of grid resolution, isotropic, even valued size.}
	
The isotropic scaling of grid resolution is shown in figure \ref{result:grid1}. We see result very similar to that of
the FFT-only test (fig \ref{result:fft}). For $n=$221, 227, 231, 235, 239, 247, 251 and 255 the simulation fails, unable
to allocate memory for the FFT work area.

Looking at figure \ref{result:grid2} we see that runtime seems to scale linearly with $n_z$, and has quadratic growth for $n_x = n_y$.
This makes sense since multiplying resolution by $a$ would result in $T(n_x \cdot n_y \cdot (a\cdot n_z)) = n \cdot T(n_x\cdot n_y \cdot n_z)$
while $T((a \cdot n_x) \cdot (a \cdot n_y) \cdot n_z) = n^2 \cdot T(n_x\cdot n_y \cdot n_z)$. For high values of both 
we see the same tendency as for isotropic scaling, where the difference between superior and inferior values becomes
dominant, giving an irregular plot.

Figures \ref{result:grid3a} and \ref{result:grid3b} show the isotropic scaling decomposed into even and odd numbers, helping
to isolate different effects. First off we see that while the shape of the plots are similar, the odd valued shows
approximately twice the runtime of the even valued plot. We can also see that only odd valued resolutions crash the
simulation. For the odd valued plot it is especially easy to isolate ideal resolutions: all the downward spikes (except
failures) occur for values of $n=3^b \cdot 5^c \cdot 7^d$, while the highest runtimes measured were for $219 = 3 \cdot 73$
and $217 = 3 \cdot 31$. Interestingly the simulation fails for 239, a prime, but appears to succeed for 241, another prime.

While the even plot shows a lot more spikes, the same holds true with regard to them being on the form $n=2^a\cdot 3^b \cdot 5^c \cdot 7^d$.
The highest measured runtime belongs to $254 = 2 \cdot 127$.

\section{Particle count}
\testresult{particles1}{Runtime as a function of the number of particles.}
\testresult{particles2}{Runtime as a function of both the number of particles and grid resolution. Isotropic scaling of resolution.}

Figure \ref{result:particles1} shows how runtime varies with the number of particles, and while it appears to fluctuate
a lot, it should be noted that the range of variation is $[1.3, 1.8]$. Compared to the grid resolution above it seems
safe to say that the number of particles has a relatively low effect on the performance of the simulation. Worthy of note
is that the plot seems to be repeating, showing the same trend for [0 - 22000], [22000 - 44000] and [44000 - 66000].

Figure \ref{result:particles2} confirms that resolution has a much larger impact on performance than particle number, and it
appears that the variations above may simply be noise. Indeed the only major variation along the particle axis is that
the simulation fails for lower resolutions with increasing particle number, and for configurations with $N_{particles}>5000$
and $n>237$. Again the simulation fails because of insufficient memory, so while particle number may not impact runtime
it has a clear impact on memory usage, and thus problem size.

\section{Solvers}
\testresult{sor}{Comparison of solvers. Runtime of each solver as a function of grid resolution, isotropically scaled.}
\testresult{fft}{FFT solver runtime by grid resolution.}

In figures \ref{result:fft} and \ref{result:sor} we see solver runtime. We see that the runtime of either solver dominates
that of the kernels (see below), explaining why $T_{simulation}(n_x, n_y, n_z) \approx T_{FFT}(n_x, n_y, n_z)$. Also of
interest, while the FFT shows it's log-linear growth, the SOR is almost constant after $n=64$.

\section{Kernels}
\subsection{determineChargesFromParticles}
\testresult{distributecharge}{Runtime of the determineChargesFromParticles() kernel, as a function of both grid resolution and number of particles.}
This kernel shows an interesting performance metric, where runtime goes down as the grid resolution increases, and is
more or less constant in the number of particles. Runtime evens out around $0.05ms$.

\subsection{electricFieldFromPotential}
\testresult{electricfield}{Runtime of the electricFieldFromPotential() kernel, as a function of both grid resolution and number of particles.}
Constant in the number of particles, runtime increases rapidly up to $0.10ms$ for $n=62$, before easing off and fluctuating between 0.10 and $0.15ms$.

\subsection{updateParticles}
\testresult{updateparticles}{Runtime of the updateParticles() kernel, as a function of both grid resolution and number of particles.}
Apart from poor performance for low grid resolutions, this kernel has a runtime that seems to be more or less independent
of both particle numerosity and grid resolution. Also has an average runtime around $0.5ms$
